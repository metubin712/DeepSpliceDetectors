{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data_generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_generator.py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from os.path import expanduser\n",
    "from pprint import pprint\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import math\n",
    "\n",
    "HOME = expanduser(\"~\")\n",
    "\n",
    "\n",
    "class TrainingSequence(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self._x, self._y = x_set, y_set\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self._x.shape[0] / self._batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self._x[idx * self._batch_size:(idx + 1) * self._batch_size]\n",
    "        batch_y = self._y[idx * self._batch_size:(idx + 1) * self._batch_size]\n",
    "\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "\n",
    "class DataGenerator:\n",
    "    \n",
    "    _data_location = f'{HOME}/.frontiers-molecular-biosciences-2020'\n",
    "    \n",
    "    # Fixed Properies\n",
    "    _data_files = [\n",
    "        'EI_true',\n",
    "        'IE_true',\n",
    "        'EI_false',\n",
    "        'IE_false'\n",
    "    ]\n",
    "    _EI_true_length = 2796\n",
    "    _IE_true_length = 2880\n",
    "    #_average_class_length = (_EI_true_length+_IE_true_length)//2\n",
    "    _false_length = 271926+329359\n",
    "    _minority_class_length = min(_EI_true_length, _IE_true_length)\n",
    "    \n",
    "    def __init__(self, seed=0, upsampling=1, validation_split=0.2):\n",
    "        # Seed value for reproducibility of random processes\n",
    "        self._seed = seed\n",
    "        self._upsampling = upsampling\n",
    "        np.random.rand(seed)\n",
    "        # Validation split's default value\n",
    "        self._validation_split = validation_split\n",
    "        # Loading the data\n",
    "        self._data = {}\n",
    "        self._validation = {'X': {}, 'y': {}}\n",
    "        self._training_org = {'X': {}, 'y': {}}\n",
    "        self._training = {'X': {}, 'y': {}}\n",
    "        self._load_files()\n",
    "        self._combine_falses()\n",
    "        self._split_data()\n",
    "        \n",
    "        # OneHotEncoder Object\n",
    "        self._X_enc = OneHotEncoder(categories=[[0.0, 1.0, 2.0, 3.0]], handle_unknown='ignore')\n",
    "        self._y_enc = OneHotEncoder(categories=[[0.0, 1.0, 2.0]], handle_unknown='ignore')\n",
    "        self._encode_data()\n",
    "        \n",
    "        # Validations do not need anymore processing. They will be combined.\n",
    "        self._combine_validation()\n",
    "        \n",
    "    def _load_files(self):\n",
    "        \"\"\"\n",
    "        Loading files into the `self._data` container\n",
    "        \"\"\"\n",
    "        for file_name in self._data_files:\n",
    "            file = np.load(f'{self._data_location}/{file_name}.npz')\n",
    "            self._data[file_name] = {\n",
    "                'X': file['X'],\n",
    "                'y': file['y']\n",
    "            }\n",
    "    \n",
    "    def _combine_falses(self):\n",
    "        # This remains\n",
    "        falses = [category for category in self._data if category.endswith('false')]\n",
    "        false_X_data = [self._data[item]['X'] for item in falses]\n",
    "        false_y_data = [self._data[item]['y'] for item in falses]\n",
    "        self._data['false'] = {\n",
    "            'X': np.concatenate(false_X_data, axis=0),\n",
    "            'y': np.concatenate(false_y_data, axis=0)\n",
    "        }\n",
    "        for item in falses:\n",
    "            del self._data[item]\n",
    "            \n",
    "    def _split_data(self):\n",
    "        \"\"\"\n",
    "        Prepare the test and validation split.\n",
    "        \"\"\"\n",
    "        valdiation_size = (int)(self._validation_split*self._minority_class_length)\n",
    "        for category in self._data.keys():\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                self._data[category]['X'], \n",
    "                self._data[category]['y'], \n",
    "                test_size=valdiation_size, \n",
    "                random_state=self._seed\n",
    "            )\n",
    "            self._training_org['X'][category] = X_train\n",
    "            self._training_org['y'][category] = y_train\n",
    "            self._validation['X'][category] = X_val\n",
    "            self._validation['y'][category] = y_val\n",
    "    \n",
    "    def _encode_data(self):\n",
    "        \"\"\"\n",
    "        Onehot encode the data\n",
    "        \"\"\"\n",
    "        for category in self._data.keys():\n",
    "            self._training_org['X'][category] = self._encode_X(self._training_org['X'][category])\n",
    "            self._training_org['y'][category] = self._encode_y(self._training_org['y'][category])\n",
    "            self._validation['X'][category] = self._encode_X(self._validation['X'][category])\n",
    "            self._validation['y'][category] = self._encode_y(self._validation['y'][category])\n",
    "    \n",
    "    def _encode_X(self, arr):\n",
    "        return np.reshape(\n",
    "            self._X_enc.fit_transform(arr.reshape((-1, 1))).toarray(), (-1, 140, 4)\n",
    "        ).astype(np.float32, copy=False)\n",
    "    \n",
    "    def _encode_y(self, arr):\n",
    "        return np.reshape(\n",
    "            self._y_enc.fit_transform(arr.reshape((-1, 1))).toarray(), (-1, 3)\n",
    "        ).astype(np.float32, copy=False)\n",
    "    \n",
    "    def _upsample(self):\n",
    "        for category in self._training_org['X']:\n",
    "            if category.endswith('false'):\n",
    "                continue\n",
    "            self._training['X'][category] = np.repeat(self._training_org['X'][category], self._upsampling, axis=0)\n",
    "            self._training['y'][category] = np.repeat(self._training_org['y'][category], self._upsampling, axis=0)\n",
    "        \n",
    "    def _downsample(self):\n",
    "        sum_of_truth = 0\n",
    "        for category in self._training_org['X']:\n",
    "            if category.endswith('true'):\n",
    "                sum_of_truth += self._training_org['X'][category].shape[0]\n",
    "        for category in self._training_org['X']:\n",
    "            if category.endswith('true'):\n",
    "                continue\n",
    "            x, y = self._unison_shuffled_copies(\n",
    "                self._training_org['X'][category], \n",
    "                self._training_org['y'][category]\n",
    "            )\n",
    "            self._training['X'][category] = x[0:(sum_of_truth//2)*self._upsampling]\n",
    "            self._training['y'][category] = y[0:(sum_of_truth//2)*self._upsampling]\n",
    "    \n",
    "    def _combine_validation(self):\n",
    "        X = np.concatenate([self._validation['X'][category] for category in self._validation['X']], axis=0)\n",
    "        y = np.concatenate([self._validation['y'][category] for category in self._validation['y']], axis=0)\n",
    "        X, y = self._unison_shuffled_copies(X, y)\n",
    "        self._validation['X'] = X\n",
    "        self._validation['y'] = y\n",
    "    \n",
    "    @staticmethod\n",
    "    def _unison_shuffled_copies(a, b):\n",
    "        assert len(a) == len(b)\n",
    "        p = np.random.permutation(len(a))\n",
    "        return a[p], b[p]\n",
    "    \n",
    "    def _combine_training(self):\n",
    "        \"\"\"\n",
    "        TODO: Combine , suffle and return train data\n",
    "        \"\"\"\n",
    "        X = np.concatenate([self._training['X'][category] for category in self._training['X']], axis=0)\n",
    "        y = np.concatenate([self._training['y'][category] for category in self._training['y']], axis=0)\n",
    "        return self._unison_shuffled_copies(X, y)\n",
    "    \n",
    "    def get_validation_data(self):\n",
    "        return self._validation['X'], self._validation['y']\n",
    "    \n",
    "    def get_training_generator(self, batch_size):\n",
    "        if self._upsampling > 1:\n",
    "            self._upsample()\n",
    "            self._downsample()\n",
    "        x, y = self._combine_training()\n",
    "        return TrainingSequence(x, y, batch_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_generator = DataGenerator(seed=0, upsampling=2, validation_split=0.2)\n",
    "    print('Processing is done!')\n",
    "    val_X, val_y = data_generator.get_validation_data()\n",
    "    print(val_X.shape, val_y.shape)\n",
    "    print()\n",
    "    i = 0\n",
    "    gen = data_generator.get_training_generator(100)\n",
    "    print(gen)\n",
    "    print(len(gen))\n",
    "    x, y = gen[0]\n",
    "    print(x.shape)\n",
    "    print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
