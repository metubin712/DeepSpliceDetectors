{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Loading necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tick\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Disabling warnings for a clean preview. Comment the next block for the default state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Necessary Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LOG_DIR = '../logs/'\n",
    "LOGS = []\n",
    "ALL_DATA = {}\n",
    "HIGH_QUALITY_IMAGES = False\n",
    "image_format = 'jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "scrolled": true
   },
   "source": [
    "Here, we list the relevant log files from the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Going through all files\n",
    "for root, dirs, files in os.walk(LOG_DIR):\n",
    "    for file in files:\n",
    "        # Ignoring unnecessary results\n",
    "        if root.find('logs') < 0:\n",
    "            continue\n",
    "        if not file.endswith('.v2'):\n",
    "            continue\n",
    "        if root.find('_level_') < 0:\n",
    "            continue\n",
    "        # Keeping the path to relevant files in memory\n",
    "        LOGS.append(f'{root}/{file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "scrolled": true
   },
   "source": [
    "Here we load the information within the log files into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the data from each relevant log file\n",
    "for log in LOGS:\n",
    "    ALL_DATA[log] = {'loss': [], 'accuracy': [], 'f1': [], 'auc_pr': [], 'start':None, 'end':None}\n",
    "    for summary in summary_iterator(log):\n",
    "        if not ALL_DATA[log]['start']:\n",
    "            ALL_DATA[log]['start'] = float(summary.wall_time)\n",
    "        ALL_DATA[log]['end'] = float(summary.wall_time)\n",
    "        if len(summary.summary.value):\n",
    "            if summary.summary.value[0].tag == 'epoch_loss':\n",
    "                ALL_DATA[log]['loss'].append(summary.summary.value[0].simple_value)\n",
    "            if summary.summary.value[0].tag == 'epoch_accuracy':\n",
    "                ALL_DATA[log]['accuracy'].append(summary.summary.value[0].simple_value)\n",
    "            if summary.summary.value[0].tag == 'epoch_f1':\n",
    "                ALL_DATA[log]['f1'].append(summary.summary.value[0].simple_value)\n",
    "            if summary.summary.value[0].tag == 'epoch_auc_pr':\n",
    "                ALL_DATA[log]['auc_pr'].append(summary.summary.value[0].simple_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load relevant data into a python array\n",
    "def get_accuracy_data_numpy(exp_keys, key_include):\n",
    "    arr = np.ndarray(shape=(0,len(ALL_DATA[exp_keys[0]]['accuracy'])))\n",
    "    for key in exp_keys:\n",
    "        if key_include not in key:\n",
    "            continue\n",
    "        arr = np.append(arr, [ALL_DATA[key]['accuracy']], axis=0)\n",
    "    return arr\n",
    "\n",
    "def get_f1_data_numpy(exp_keys, key_include):\n",
    "    arr = np.ndarray(shape=(0,len(ALL_DATA[exp_keys[0]]['f1'])))\n",
    "    for key in exp_keys:\n",
    "        if key_include not in key:\n",
    "            continue\n",
    "        arr = np.append(arr, [ALL_DATA[key]['f1']], axis=0)\n",
    "    return arr\n",
    "\n",
    "def get_auc_pr_data_numpy(exp_keys, key_include):\n",
    "    arr = np.ndarray(shape=(0,len(ALL_DATA[exp_keys[0]]['auc_pr'])))\n",
    "    for key in exp_keys:\n",
    "        if key_include not in key:\n",
    "            continue\n",
    "        arr = np.append(arr, [ALL_DATA[key]['auc_pr']], axis=0)\n",
    "    return arr\n",
    "\n",
    "# Function to load relevant time data\n",
    "def get_time_in_seconds(exp_keys, key_include):\n",
    "    times = []\n",
    "    for key in exp_keys:\n",
    "        if key_include not in key:\n",
    "            continue\n",
    "        times.append(ALL_DATA[key]['end']-ALL_DATA[key]['start'])\n",
    "    return times\n",
    "\n",
    "# Function for creating accuracy per epoch plots\n",
    "def plot_all_and_average_accuracy(exp_keys, title=None, save=False):\n",
    "    fig = plt.figure(dpi=300 if HIGH_QUALITY_IMAGES else 72, figsize=(7, 6))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.rcParams['savefig.facecolor'] = 'w'\n",
    "\n",
    "    ax = fig.gca()\n",
    "    ax.yaxis.set_major_formatter(tick.FuncFormatter(lambda x,y: '{:.0f} %'.format(x*100)))\n",
    "\n",
    "    data_train = get_accuracy_data_numpy(exp_keys, 'train')\n",
    "    data_valid = get_accuracy_data_numpy(exp_keys, 'valid')\n",
    "    linewidth = 2\n",
    "\n",
    "    for row in data_train:\n",
    "        plt.plot(row, color='#E2EDF3', linewidth=linewidth)\n",
    "    for row in data_valid:\n",
    "        plt.plot(row, color='#E2F3F2', linewidth=linewidth)\n",
    "    plt.plot(np.mean(data_train, axis=0), color='#4889AD', label='Average Training Accuracy', linewidth=linewidth)\n",
    "    plt.plot(np.mean(data_valid, axis=0), color='#52B7B0',label='Average Validation Accuracy', linewidth=linewidth)\n",
    "\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend(loc='lower right')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# Function for creating f1 per epoch plots\n",
    "def plot_all_and_average_f1(exp_keys, title=None, save=False):\n",
    "    fig = plt.figure(dpi=300 if HIGH_QUALITY_IMAGES else 72, figsize=(7, 6))\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.rcParams['savefig.facecolor'] = 'w'\n",
    "\n",
    "    ax = fig.gca()\n",
    "    # ax.yaxis.set_major_formatter(tick.FuncFormatter(lambda x,y: '{:.0f}'.format(x*100)))\n",
    "\n",
    "    data_train = get_f1_data_numpy(exp_keys, 'train')\n",
    "    data_valid = get_f1_data_numpy(exp_keys, 'valid')\n",
    "    linewidth = 2\n",
    "\n",
    "    for row in data_train:\n",
    "        plt.plot(row, color='#E2EDF3', linewidth=linewidth)\n",
    "    for row in data_valid:\n",
    "        plt.plot(row, color='#E2F3F2', linewidth=linewidth)\n",
    "    plt.plot(np.mean(data_train, axis=0), color='#4889AD', label='Average Training F1 Score', linewidth=linewidth)\n",
    "    plt.plot(np.mean(data_valid, axis=0), color='#52B7B0',label='Average Validation F1 Score', linewidth=linewidth)\n",
    "\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend(loc='lower right')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# Function for creating AUC PR per epoch plots\n",
    "def plot_all_and_average_auc_pr(exp_keys, title=None, save=False):\n",
    "    fig = plt.figure(dpi=300 if HIGH_QUALITY_IMAGES else 72, figsize=(7, 6))\n",
    "    plt.ylabel('AUC PR')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.rcParams['savefig.facecolor'] = 'w'\n",
    "\n",
    "    ax = fig.gca()\n",
    "    # ax.yaxis.set_major_formatter(tick.FuncFormatter(lambda x,y: '{:.0f}'.format(x*100)))\n",
    "\n",
    "    data_train = get_auc_pr_data_numpy(exp_keys, 'train')\n",
    "    data_valid = get_auc_pr_data_numpy(exp_keys, 'valid')\n",
    "    linewidth = 2\n",
    "\n",
    "    for row in data_train:\n",
    "        plt.plot(row, color='#E2EDF3', linewidth=linewidth)\n",
    "    for row in data_valid:\n",
    "        plt.plot(row, color='#E2F3F2', linewidth=linewidth)\n",
    "    plt.plot(np.mean(data_train, axis=0), color='#4889AD', label='Average Training AUC PR', linewidth=linewidth)\n",
    "    plt.plot(np.mean(data_valid, axis=0), color='#52B7B0',label='Average Validation AUC PR', linewidth=linewidth)\n",
    "\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend(loc='lower right')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Creating Accuracy Per Epoch Plots for CNN Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Experiment 1\n",
    "\n",
    "cnn1 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'cnn_level_1/' in key:\n",
    "        cnn1.append(key)\n",
    "\n",
    "cnn1_train_time = np.mean(get_time_in_seconds(cnn1, 'train'))\n",
    "cnn1_valid_time = np.mean(get_time_in_seconds(cnn1, 'valid'))\n",
    "print(f'Average Training Time: {cnn1_train_time}, Average Validation Time: {cnn1_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(cnn1, 'CNN / 1 Layer', save=f'cnn1_acc.{image_format}')\n",
    "plot_all_and_average_f1(cnn1, 'CNN / 1 Layer', save=f'cnn1_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(cnn1, 'CNN / 1 Layer', save=f'cnn1_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Experiment 2\n",
    "\n",
    "cnn2 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'cnn_level_2/' in key:\n",
    "        cnn2.append(key)\n",
    "\n",
    "cnn2_train_time = np.mean(get_time_in_seconds(cnn2, 'train'))\n",
    "cnn2_valid_time = np.mean(get_time_in_seconds(cnn2, 'valid'))\n",
    "print(f'Average Training Time: {cnn2_train_time}, Average Validation Time: {cnn2_valid_time}')\n",
    "\n",
    "\n",
    "plot_all_and_average_accuracy(cnn2, 'CNN / 2 Layers', save=f'cnn2_acc.{image_format}')\n",
    "plot_all_and_average_f1(cnn2, 'CNN / 2 Layers', save=f'cnn2_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(cnn2, 'CNN / 2 Layers', save=f'cnn2_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Experiment 3\n",
    "\n",
    "cnn3 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'cnn_level_3/' in key:\n",
    "        cnn3.append(key)\n",
    "        \n",
    "cnn3_train_time = np.mean(get_time_in_seconds(cnn3, 'train'))\n",
    "cnn3_valid_time = np.mean(get_time_in_seconds(cnn3, 'valid'))\n",
    "print(f'Average Training Time: {cnn3_train_time}, Average Validation Time: {cnn3_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(cnn3, 'CNN / 3 Layers', save=f'cnn3_acc.{image_format}')\n",
    "plot_all_and_average_f1(cnn3, 'CNN / 3 Layers', save=f'cnn3_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(cnn3, 'CNN / 3 Layers', save=f'cnn3_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Experiment 4\n",
    "\n",
    "cnn4 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'cnn_level_4/' in key:\n",
    "        cnn4.append(key)\n",
    "        \n",
    "cnn4_train_time = np.mean(get_time_in_seconds(cnn4, 'train'))\n",
    "cnn4_valid_time = np.mean(get_time_in_seconds(cnn4, 'valid'))\n",
    "print(f'Average Training Time: {cnn4_train_time}, Average Validation Time: {cnn4_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(cnn4, 'CNN / 4 Layers', save=f'cnn4_acc.{image_format}')\n",
    "plot_all_and_average_f1(cnn4, 'CNN / 4 Layers', save=f'cnn4_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(cnn4, 'CNN / 4 Layers', save=f'cnn4_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Experiment 5\n",
    "\n",
    "cnn5 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'cnn_level_5/' in key:\n",
    "        cnn5.append(key)\n",
    "        \n",
    "cnn5_train_time = np.mean(get_time_in_seconds(cnn5, 'train'))\n",
    "cnn5_valid_time = np.mean(get_time_in_seconds(cnn5, 'valid'))\n",
    "print(f'Average Training Time: {cnn5_train_time}, Average Validation Time: {cnn5_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(cnn5, 'CNN / 5 Layers', save=f'cnn5_acc.{image_format}')\n",
    "plot_all_and_average_f1(cnn5, 'CNN / 5 Layers', save=f'cnn5_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(cnn5, 'CNN / 5 Layers', save=f'cnn5_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Saving the maximum average accuracy and run time per experiment for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data_layer_1 = get_accuracy_data_numpy(cnn1, 'valid')\n",
    "acc_data_layer_2 = get_accuracy_data_numpy(cnn2, 'valid')\n",
    "acc_data_layer_3 = get_accuracy_data_numpy(cnn3, 'valid')\n",
    "acc_data_layer_4 = get_accuracy_data_numpy(cnn4, 'valid')\n",
    "acc_data_layer_5 = get_accuracy_data_numpy(cnn5, 'valid')\n",
    "\n",
    "acc_mean_1 = np.max(np.mean(acc_data_layer_1, axis=0))\n",
    "acc_mean_2 = np.max(np.mean(acc_data_layer_2, axis=0))\n",
    "acc_mean_3 = np.max(np.mean(acc_data_layer_3, axis=0))\n",
    "acc_mean_4 = np.max(np.mean(acc_data_layer_4, axis=0))\n",
    "acc_mean_5 = np.max(np.mean(acc_data_layer_5, axis=0))\n",
    "acc_cnn_arr_mean = [acc_mean_1, acc_mean_2, acc_mean_3, acc_mean_4, acc_mean_5]\n",
    "\n",
    "acc_max_1 = np.max(acc_data_layer_1)\n",
    "acc_max_2 = np.max(acc_data_layer_2)\n",
    "acc_max_3 = np.max(acc_data_layer_3)\n",
    "acc_max_4 = np.max(acc_data_layer_4)\n",
    "acc_max_5 = np.max(acc_data_layer_5)\n",
    "acc_cnn_arr_max = [acc_max_1, acc_max_2, acc_max_3, acc_max_4, acc_max_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_data_layer_1 = get_f1_data_numpy(cnn1, 'valid')\n",
    "f1_data_layer_2 = get_f1_data_numpy(cnn2, 'valid')\n",
    "f1_data_layer_3 = get_f1_data_numpy(cnn3, 'valid')\n",
    "f1_data_layer_4 = get_f1_data_numpy(cnn4, 'valid')\n",
    "f1_data_layer_5 = get_f1_data_numpy(cnn5, 'valid')\n",
    "\n",
    "f1_mean_1 = np.max(np.mean(f1_data_layer_1, axis=0))\n",
    "f1_mean_2 = np.max(np.mean(f1_data_layer_2, axis=0))\n",
    "f1_mean_3 = np.max(np.mean(f1_data_layer_3, axis=0))\n",
    "f1_mean_4 = np.max(np.mean(f1_data_layer_4, axis=0))\n",
    "f1_mean_5 = np.max(np.mean(f1_data_layer_5, axis=0))\n",
    "f1_cnn_arr_mean = [f1_mean_1, f1_mean_2, f1_mean_3, f1_mean_4, f1_mean_5]\n",
    "\n",
    "f1_max_1 = np.max(f1_data_layer_1)\n",
    "f1_max_2 = np.max(f1_data_layer_2)\n",
    "f1_max_3 = np.max(f1_data_layer_3)\n",
    "f1_max_4 = np.max(f1_data_layer_4)\n",
    "f1_max_5 = np.max(f1_data_layer_5)\n",
    "f1_cnn_arr_max = [f1_max_1, f1_max_2, f1_max_3, f1_max_4, f1_max_5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_pr_data_layer_1 = get_auc_pr_data_numpy(cnn1, 'valid')\n",
    "auc_pr_data_layer_2 = get_auc_pr_data_numpy(cnn2, 'valid')\n",
    "auc_pr_data_layer_3 = get_auc_pr_data_numpy(cnn3, 'valid')\n",
    "auc_pr_data_layer_4 = get_auc_pr_data_numpy(cnn4, 'valid')\n",
    "auc_pr_data_layer_5 = get_auc_pr_data_numpy(cnn5, 'valid')\n",
    "\n",
    "auc_pr_mean_1 = np.max(np.mean(auc_pr_data_layer_1, axis=0))\n",
    "auc_pr_mean_2 = np.max(np.mean(auc_pr_data_layer_2, axis=0))\n",
    "auc_pr_mean_3 = np.max(np.mean(auc_pr_data_layer_3, axis=0))\n",
    "auc_pr_mean_4 = np.max(np.mean(auc_pr_data_layer_4, axis=0))\n",
    "auc_pr_mean_5 = np.max(np.mean(auc_pr_data_layer_5, axis=0))\n",
    "auc_pr_cnn_arr_mean = [auc_pr_mean_1, auc_pr_mean_2, auc_pr_mean_3, auc_pr_mean_4, auc_pr_mean_5]\n",
    "\n",
    "auc_pr_max_1 = np.max(auc_pr_data_layer_1)\n",
    "auc_pr_max_2 = np.max(auc_pr_data_layer_2)\n",
    "auc_pr_max_3 = np.max(auc_pr_data_layer_3)\n",
    "auc_pr_max_4 = np.max(auc_pr_data_layer_4)\n",
    "auc_pr_max_5 = np.max(auc_pr_data_layer_5)\n",
    "auc_pr_cnn_arr_max = [auc_pr_max_1, auc_pr_max_2, auc_pr_max_3, auc_pr_max_4, auc_pr_max_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cnn_time = [cnn1_train_time, cnn2_train_time, cnn3_train_time, cnn4_train_time, cnn5_train_time]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Creating Accuracy Per Epoch Plots for BLSTM Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLSTM Experiment 1\n",
    "\n",
    "blstm1 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'blstm_level_1/' in key:\n",
    "        blstm1.append(key)\n",
    "        \n",
    "blstm1_train_time = np.mean(get_time_in_seconds(blstm1, 'train'))\n",
    "blstm1_valid_time = np.mean(get_time_in_seconds(blstm1, 'valid'))\n",
    "print(f'Average Training Time: {blstm1_train_time}, Average Validation Time: {blstm1_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(blstm1, 'BLSTM / 1 Layers', save=f'blstm1_acc.{image_format}')\n",
    "plot_all_and_average_f1(blstm1, 'BLSTM / 1 Layers', save=f'blstm1_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(blstm1, 'BLSTM / 1 Layers', save=f'blstm1_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLSTM Experiment 2\n",
    "\n",
    "blstm2 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'blstm_level_2/' in key:\n",
    "        blstm2.append(key)\n",
    "        \n",
    "blstm2_train_time = np.mean(get_time_in_seconds(blstm2, 'train'))\n",
    "blstm2_valid_time = np.mean(get_time_in_seconds(blstm2, 'valid'))\n",
    "print(f'Average Training Time: {blstm2_train_time}, Average Validation Time: {blstm2_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(blstm2, 'BLSTM / 2 Layers', save=f'blstm2_acc.{image_format}')\n",
    "plot_all_and_average_f1(blstm2, 'BLSTM / 2 Layers', save=f'blstm2_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(blstm2, 'BLSTM / 2 Layers', save=f'blstm2_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLSTM Experiment 3\n",
    "\n",
    "blstm3 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'blstm_level_3/' in key:\n",
    "        blstm3.append(key)\n",
    "        \n",
    "blstm3_train_time = np.mean(get_time_in_seconds(blstm3, 'train'))\n",
    "blstm3_valid_time = np.mean(get_time_in_seconds(blstm3, 'valid'))\n",
    "print(f'Average Training Time: {blstm3_train_time}, Average Validation Time: {blstm3_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(blstm3,'BLSTM / 3 Layers', save=f'blstm3_acc.{image_format}')\n",
    "plot_all_and_average_f1(blstm3,'BLSTM / 3 Layers', save=f'blstm3_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(blstm3,'BLSTM / 3 Layers', save=f'blstm3_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLSTM Experiment 4\n",
    "\n",
    "blstm4 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'blstm_level_4/' in key:\n",
    "        blstm4.append(key)\n",
    "        \n",
    "blstm4_train_time = np.mean(get_time_in_seconds(blstm4, 'train'))\n",
    "blstm4_valid_time = np.mean(get_time_in_seconds(blstm4, 'valid'))\n",
    "print(f'Average Training Time: {blstm4_train_time}, Average Validation Time: {blstm4_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(blstm4, 'BLSTM / 4 Layers', save=f'blstm4_acc.{image_format}')\n",
    "plot_all_and_average_f1(blstm4, 'BLSTM / 4 Layers', save=f'blstm4_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(blstm4, 'BLSTM / 4 Layers', save=f'blstm4_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLSTM Experiment 5\n",
    "\n",
    "blstm5 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'blstm_level_5/' in key:\n",
    "        blstm5.append(key)\n",
    "        \n",
    "blstm5_train_time = np.mean(get_time_in_seconds(blstm5, 'train'))\n",
    "blstm5_valid_time = np.mean(get_time_in_seconds(blstm5, 'valid'))\n",
    "print(f'Average Training Time: {blstm5_train_time}, Average Validation Time: {blstm5_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(blstm5, 'BLSTM / 5 Layers', save=f'blstm5_acc.{image_format}')\n",
    "plot_all_and_average_f1(blstm5, 'BLSTM / 5 Layers', save=f'blstm5_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(blstm5, 'BLSTM / 5 Layers', save=f'blstm5_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Saving the maximum average accuracy and run time per experiment for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data_layer_1 = get_accuracy_data_numpy(blstm1, 'valid')\n",
    "acc_data_layer_2 = get_accuracy_data_numpy(blstm2, 'valid')\n",
    "acc_data_layer_3 = get_accuracy_data_numpy(blstm3, 'valid')\n",
    "acc_data_layer_4 = get_accuracy_data_numpy(blstm4, 'valid')\n",
    "acc_data_layer_5 = get_accuracy_data_numpy(blstm5, 'valid')\n",
    "\n",
    "acc_mean_1 = np.max(np.mean(acc_data_layer_1, axis=0))\n",
    "acc_mean_2 = np.max(np.mean(acc_data_layer_2, axis=0))\n",
    "acc_mean_3 = np.max(np.mean(acc_data_layer_3, axis=0))\n",
    "acc_mean_4 = np.max(np.mean(acc_data_layer_4, axis=0))\n",
    "acc_mean_5 = np.max(np.mean(acc_data_layer_5, axis=0))\n",
    "acc_blstm_arr_mean = [acc_mean_1, acc_mean_2, acc_mean_3, acc_mean_4, acc_mean_5]\n",
    "\n",
    "acc_max_1 = np.max(acc_data_layer_1)\n",
    "acc_max_2 = np.max(acc_data_layer_2)\n",
    "acc_max_3 = np.max(acc_data_layer_3)\n",
    "acc_max_4 = np.max(acc_data_layer_4)\n",
    "acc_max_5 = np.max(acc_data_layer_5)\n",
    "acc_blstm_arr_max = [acc_max_1, acc_max_2, acc_max_3, acc_max_4, acc_max_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_data_layer_1 = get_f1_data_numpy(blstm1, 'valid')\n",
    "f1_data_layer_2 = get_f1_data_numpy(blstm2, 'valid')\n",
    "f1_data_layer_3 = get_f1_data_numpy(blstm3, 'valid')\n",
    "f1_data_layer_4 = get_f1_data_numpy(blstm4, 'valid')\n",
    "f1_data_layer_5 = get_f1_data_numpy(blstm5, 'valid')\n",
    "\n",
    "f1_mean_1 = np.max(np.mean(f1_data_layer_1, axis=0))\n",
    "f1_mean_2 = np.max(np.mean(f1_data_layer_2, axis=0))\n",
    "f1_mean_3 = np.max(np.mean(f1_data_layer_3, axis=0))\n",
    "f1_mean_4 = np.max(np.mean(f1_data_layer_4, axis=0))\n",
    "f1_mean_5 = np.max(np.mean(f1_data_layer_5, axis=0))\n",
    "f1_blstm_arr_mean = [f1_mean_1, f1_mean_2, f1_mean_3, f1_mean_4, f1_mean_5]\n",
    "\n",
    "f1_max_1 = np.max(f1_data_layer_1)\n",
    "f1_max_2 = np.max(f1_data_layer_2)\n",
    "f1_max_3 = np.max(f1_data_layer_3)\n",
    "f1_max_4 = np.max(f1_data_layer_4)\n",
    "f1_max_5 = np.max(f1_data_layer_5)\n",
    "f1_blstm_arr_max = [f1_max_1, f1_max_2, f1_max_3, f1_max_4, f1_max_5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_pr_data_layer_1 = get_auc_pr_data_numpy(blstm1, 'valid')\n",
    "auc_pr_data_layer_2 = get_auc_pr_data_numpy(blstm2, 'valid')\n",
    "auc_pr_data_layer_3 = get_auc_pr_data_numpy(blstm3, 'valid')\n",
    "auc_pr_data_layer_4 = get_auc_pr_data_numpy(blstm4, 'valid')\n",
    "auc_pr_data_layer_5 = get_auc_pr_data_numpy(blstm5, 'valid')\n",
    "\n",
    "auc_pr_mean_1 = np.max(np.mean(auc_pr_data_layer_1, axis=0))\n",
    "auc_pr_mean_2 = np.max(np.mean(auc_pr_data_layer_2, axis=0))\n",
    "auc_pr_mean_3 = np.max(np.mean(auc_pr_data_layer_3, axis=0))\n",
    "auc_pr_mean_4 = np.max(np.mean(auc_pr_data_layer_4, axis=0))\n",
    "auc_pr_mean_5 = np.max(np.mean(auc_pr_data_layer_5, axis=0))\n",
    "auc_pr_blstm_arr_mean = [auc_pr_mean_1, auc_pr_mean_2, auc_pr_mean_3, auc_pr_mean_4, auc_pr_mean_5]\n",
    "\n",
    "auc_pr_max_1 = np.max(auc_pr_data_layer_1)\n",
    "auc_pr_max_2 = np.max(auc_pr_data_layer_2)\n",
    "auc_pr_max_3 = np.max(auc_pr_data_layer_3)\n",
    "auc_pr_max_4 = np.max(auc_pr_data_layer_4)\n",
    "auc_pr_max_5 = np.max(auc_pr_data_layer_5)\n",
    "auc_pr_blstm_arr_max = [auc_pr_max_1, auc_pr_max_2, auc_pr_max_3, auc_pr_max_4, auc_pr_max_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "blstm_time = [blstm1_train_time, blstm2_train_time, blstm3_train_time, blstm4_train_time, blstm5_train_time]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Creating Accuracy Per Epoch Plots for BLSTM Extended Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLSTM Extended Experiment 1\n",
    "\n",
    "blstm1 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'blstm_level_1_extended/' in key:\n",
    "        blstm1.append(key)\n",
    "        \n",
    "blstm1_train_time = np.mean(get_time_in_seconds(blstm1, 'train'))\n",
    "blstm1_valid_time = np.mean(get_time_in_seconds(blstm1, 'valid'))\n",
    "print(f'Average Training Time: {blstm1_train_time}, Average Validation Time: {blstm1_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(blstm1, 'BLSTM (Extended) / 1 Layer', save=f'blstm1_ext_acc.{image_format}')\n",
    "plot_all_and_average_f1(blstm1, 'BLSTM (Extended) / 1 Layer', save=f'blstm1_ext_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(blstm1, 'BLSTM (Extended) / 1 Layer', save=f'blstm1_ext_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLSTM Extended Experiment 2\n",
    "\n",
    "blstm2 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'blstm_level_2_extended/' in key:\n",
    "        blstm2.append(key)\n",
    "        \n",
    "blstm2_train_time = np.mean(get_time_in_seconds(blstm2, 'train'))\n",
    "blstm2_valid_time = np.mean(get_time_in_seconds(blstm2, 'valid'))\n",
    "print(f'Average Training Time: {blstm2_train_time}, Average Validation Time: {blstm2_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(blstm2,'BLSTM (Extended) / 2 Layers', save=f'blstm2_ext_acc.{image_format}')\n",
    "plot_all_and_average_f1(blstm2,'BLSTM (Extended) / 2 Layers', save=f'blstm2_ext_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(blstm2,'BLSTM (Extended) / 2 Layers', save=f'blstm2_ext_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLSTM Extended Experiment 3\n",
    "\n",
    "blstm3 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'blstm_level_3_extended/' in key:\n",
    "        blstm3.append(key)\n",
    "        \n",
    "blstm3_train_time = np.mean(get_time_in_seconds(blstm3, 'train'))\n",
    "blstm3_valid_time = np.mean(get_time_in_seconds(blstm3, 'valid'))\n",
    "print(f'Average Training Time: {blstm3_train_time}, Average Validation Time: {blstm3_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(blstm3, 'BLSTM (Extended) / 3 Layers', save=f'blstm3_ext_acc.{image_format}')\n",
    "plot_all_and_average_f1(blstm3, 'BLSTM (Extended) / 3 Layers', save=f'blstm3_ext_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(blstm3, 'BLSTM (Extended) / 3 Layers', save=f'blstm3_ext_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLSTM Extended Experiment 4\n",
    "\n",
    "blstm4 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'blstm_level_4_extended/' in key:\n",
    "        blstm4.append(key)\n",
    "        \n",
    "blstm4_train_time = np.mean(get_time_in_seconds(blstm4, 'train'))\n",
    "blstm4_valid_time = np.mean(get_time_in_seconds(blstm4, 'valid'))\n",
    "print(f'Average Training Time: {blstm4_train_time}, Average Validation Time: {blstm4_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(blstm4, 'BLSTM (Extended) / 4 Layers', save=f'blstm4_ext_acc.{image_format}')\n",
    "plot_all_and_average_f1(blstm4, 'BLSTM (Extended) / 4 Layers', save=f'blstm4_ext_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(blstm4, 'BLSTM (Extended) / 4 Layers', save=f'blstm4_ext_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLSTM Extended Experiment 5\n",
    "\n",
    "blstm5 = []\n",
    "for key in ALL_DATA.keys():\n",
    "    if 'blstm_level_5_extended/' in key:\n",
    "        blstm5.append(key)\n",
    "        \n",
    "blstm5_train_time = np.mean(get_time_in_seconds(blstm5, 'train'))\n",
    "blstm5_valid_time = np.mean(get_time_in_seconds(blstm5, 'valid'))\n",
    "print(f'Average Training Time: {blstm5_train_time}, Average Validation Time: {blstm5_valid_time}')\n",
    "\n",
    "plot_all_and_average_accuracy(blstm5, 'BLSTM (Extended) / 5 Layers', save=f'blstm5_ext_acc.{image_format}')\n",
    "plot_all_and_average_f1(blstm5, 'BLSTM (Extended) / 5 Layers', save=f'blstm5_ext_f1.{image_format}')\n",
    "plot_all_and_average_auc_pr(blstm5, 'BLSTM (Extended) / 5 Layers', save=f'blstm5_ext_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Saving the maximum average accuracy and run time per experiment for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data_layer_1 = get_f1_data_numpy(blstm1, 'valid')\n",
    "acc_data_layer_2 = get_f1_data_numpy(blstm2, 'valid')\n",
    "acc_data_layer_3 = get_f1_data_numpy(blstm3, 'valid')\n",
    "acc_data_layer_4 = get_f1_data_numpy(blstm4, 'valid')\n",
    "acc_data_layer_5 = get_f1_data_numpy(blstm5, 'valid')\n",
    "\n",
    "acc_mean_1 = np.max(np.mean(acc_data_layer_1, axis=0))\n",
    "acc_mean_2 = np.max(np.mean(acc_data_layer_2, axis=0))\n",
    "acc_mean_3 = np.max(np.mean(acc_data_layer_3, axis=0))\n",
    "acc_mean_4 = np.max(np.mean(acc_data_layer_4, axis=0))\n",
    "acc_mean_5 = np.max(np.mean(acc_data_layer_5, axis=0))\n",
    "acc_blstm_ext_arr_mean = [acc_mean_1, acc_mean_2, acc_mean_3, acc_mean_4, acc_mean_5]\n",
    "\n",
    "acc_max_1 = np.max(acc_data_layer_1)\n",
    "acc_max_2 = np.max(acc_data_layer_2)\n",
    "acc_max_3 = np.max(acc_data_layer_3)\n",
    "acc_max_4 = np.max(acc_data_layer_4)\n",
    "acc_max_5 = np.max(acc_data_layer_5)\n",
    "acc_blstm_ext_arr_max = [acc_max_1, acc_max_2, acc_max_3, acc_max_4, acc_max_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_data_layer_1 = get_f1_data_numpy(blstm1, 'valid')\n",
    "f1_data_layer_2 = get_f1_data_numpy(blstm2, 'valid')\n",
    "f1_data_layer_3 = get_f1_data_numpy(blstm3, 'valid')\n",
    "f1_data_layer_4 = get_f1_data_numpy(blstm4, 'valid')\n",
    "f1_data_layer_5 = get_f1_data_numpy(blstm5, 'valid')\n",
    "\n",
    "f1_mean_1 = np.max(np.mean(f1_data_layer_1, axis=0))\n",
    "f1_mean_2 = np.max(np.mean(f1_data_layer_2, axis=0))\n",
    "f1_mean_3 = np.max(np.mean(f1_data_layer_3, axis=0))\n",
    "f1_mean_4 = np.max(np.mean(f1_data_layer_4, axis=0))\n",
    "f1_mean_5 = np.max(np.mean(f1_data_layer_5, axis=0))\n",
    "f1_blstm_ext_arr_mean = [f1_mean_1, f1_mean_2, f1_mean_3, f1_mean_4, f1_mean_5]\n",
    "\n",
    "f1_max_1 = np.max(f1_data_layer_1)\n",
    "f1_max_2 = np.max(f1_data_layer_2)\n",
    "f1_max_3 = np.max(f1_data_layer_3)\n",
    "f1_max_4 = np.max(f1_data_layer_4)\n",
    "f1_max_5 = np.max(f1_data_layer_5)\n",
    "f1_blstm_ext_arr_max = [f1_max_1, f1_max_2, f1_max_3, f1_max_4, f1_max_5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_pr_data_layer_1 = get_auc_pr_data_numpy(blstm1, 'valid')\n",
    "auc_pr_data_layer_2 = get_auc_pr_data_numpy(blstm2, 'valid')\n",
    "auc_pr_data_layer_3 = get_auc_pr_data_numpy(blstm3, 'valid')\n",
    "auc_pr_data_layer_4 = get_auc_pr_data_numpy(blstm4, 'valid')\n",
    "auc_pr_data_layer_5 = get_auc_pr_data_numpy(blstm5, 'valid')\n",
    "\n",
    "auc_pr_mean_1 = np.max(np.mean(auc_pr_data_layer_1, axis=0))\n",
    "auc_pr_mean_2 = np.max(np.mean(auc_pr_data_layer_2, axis=0))\n",
    "auc_pr_mean_3 = np.max(np.mean(auc_pr_data_layer_3, axis=0))\n",
    "auc_pr_mean_4 = np.max(np.mean(auc_pr_data_layer_4, axis=0))\n",
    "auc_pr_mean_5 = np.max(np.mean(auc_pr_data_layer_5, axis=0))\n",
    "auc_pr_blstm_ext_arr_mean = [auc_pr_mean_1, auc_pr_mean_2, auc_pr_mean_3, auc_pr_mean_4, auc_pr_mean_5]\n",
    "\n",
    "auc_pr_max_1 = np.max(auc_pr_data_layer_1)\n",
    "auc_pr_max_2 = np.max(auc_pr_data_layer_2)\n",
    "auc_pr_max_3 = np.max(auc_pr_data_layer_3)\n",
    "auc_pr_max_4 = np.max(auc_pr_data_layer_4)\n",
    "auc_pr_max_5 = np.max(auc_pr_data_layer_5)\n",
    "auc_pr_blstm_ext_arr_max = [auc_pr_max_1, auc_pr_max_2, auc_pr_max_3, auc_pr_max_4, auc_pr_max_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "blstm_ext_time = [blstm1_train_time, blstm2_train_time, blstm3_train_time, blstm4_train_time, blstm5_train_time]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Helper function to plot average training time and average accuracy per increasing layers for different experiment types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['One Layer', 'Two Layers', 'Three Layers', 'Four Layers', 'Five Layers']\n",
    "colors = ['#4889AD', '#4889AD', '#4889AD']\n",
    "markers = ['d', 's', 'o']\n",
    "linestyles = ['-', '--', '-.']\n",
    "x = [0, 1, 2, 3, 4]\n",
    "\n",
    "def plot_time_per_layer(data, save=False):\n",
    "    fig = plt.figure(dpi=300 if HIGH_QUALITY_IMAGES else 72, figsize=(7, 6))\n",
    "    plt.xticks(x, labels, rotation='0')\n",
    "    plt.ylabel('Average Training Time (Seconds)')\n",
    "    plt.rcParams['savefig.facecolor'] = 'w'\n",
    "    plt.grid(axis='y')\n",
    "    plt.grid(axis='y', which='minor', lw=0.5, c='#ccc')\n",
    "    ax = fig.gca()\n",
    "    ax.set_yscale('log')\n",
    "    ax.yaxis.set_major_formatter(tick.FuncFormatter(lambda x,y: f'{x:.0f}'))\n",
    "    ax.yaxis.set_minor_formatter(tick.FuncFormatter(lambda x,y: f'{x:.0f}'))\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=5)\n",
    "    for idx, item in enumerate(data):\n",
    "        plt.plot(item['list'], label=item['name'], color=colors[idx], linestyle=linestyles[idx], marker=markers[idx])\n",
    "    plt.legend(loc = 'right', bbox_to_anchor=(1, 0.3))\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_accuracy_per_layer(data, save=False):\n",
    "    fig = plt.figure(dpi=300 if HIGH_QUALITY_IMAGES else 72, figsize=(7, 6))\n",
    "    plt.xticks(x, labels, rotation='0')\n",
    "    plt.ylabel('Average Accuracy')\n",
    "    plt.rcParams['savefig.facecolor'] = 'w'\n",
    "    plt.grid(axis='y')\n",
    "    ax = fig.gca()\n",
    "    ax.set_yticks(np.arange(0, 1., 0.01))\n",
    "    ax.yaxis.set_major_formatter(tick.FuncFormatter(lambda x,y: f'{y} %'))\n",
    "    for idx, item in enumerate(data):\n",
    "        plt.plot(item['list'], label=item['name'], color=colors[idx], linestyle=linestyles[idx], marker=markers[idx])\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_f1_per_layer(data, save=False):\n",
    "    fig = plt.figure(dpi=300 if HIGH_QUALITY_IMAGES else 72, figsize=(7, 6))\n",
    "    plt.xticks(x, labels, rotation='0')\n",
    "    plt.ylabel('Average F1 Score')\n",
    "    plt.rcParams['savefig.facecolor'] = 'w'\n",
    "    plt.grid(axis='y')\n",
    "    ax = fig.gca()\n",
    "    ax.set_yticks(np.arange(0, 1., 0.01))\n",
    "    for idx, item in enumerate(data):\n",
    "        plt.plot(item['list'], label=item['name'], color=colors[idx], linestyle=linestyles[idx], marker=markers[idx])\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_auc_pr_per_layer(data, save=False):\n",
    "    fig = plt.figure(dpi=300 if HIGH_QUALITY_IMAGES else 72, figsize=(7, 6))\n",
    "    plt.xticks(x, labels, rotation='0')\n",
    "    plt.ylabel('Average AUC PR')\n",
    "    plt.rcParams['savefig.facecolor'] = 'w'\n",
    "    plt.grid(axis='y')\n",
    "    ax = fig.gca()\n",
    "    ax.set_yticks(np.arange(0, 1., 0.01))\n",
    "    for idx, item in enumerate(data):\n",
    "        plt.plot(item['list'], label=item['name'], color=colors[idx], linestyle=linestyles[idx], marker=markers[idx])\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(save)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Creating average per increasing layer plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'name': 'CNN (Kernel size: 3, Termination Epoch: 300)' ,'list': cnn_time},\n",
    "    {'name': 'BLSTM (Termination Epoch: 300)' ,'list': blstm_time},\n",
    "    {'name': 'BLSTM (Termination Epoch: 1000)' ,'list': blstm_ext_time}\n",
    "]\n",
    "plot_time_per_layer(data, save=f'avg_time.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'name': 'CNN (Kernel size: 3, Termination Epoch: 300)' ,'list': acc_cnn_arr_mean},\n",
    "    {'name': 'BLSTM (Termination Epoch: 300)' ,'list': acc_blstm_arr_mean},\n",
    "    {'name': 'BLSTM (Termination Epoch: 1000)' ,'list': acc_blstm_ext_arr_mean}\n",
    "]\n",
    "plot_accuracy_per_layer(data, save=f'avg_acc.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'name': 'CNN (Kernel size: 3, Termination Epoch: 300)' ,'list': f1_cnn_arr_mean},\n",
    "    {'name': 'BLSTM (Termination Epoch: 300)' ,'list': f1_blstm_arr_mean},\n",
    "    {'name': 'BLSTM (Termination Epoch: 1000)' ,'list': f1_blstm_ext_arr_mean}\n",
    "]\n",
    "plot_f1_per_layer(data, save=f'avg_f1.{image_format}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'name': 'CNN (Kernel size: 3, Termination Epoch: 300)' ,'list': auc_pr_cnn_arr_mean},\n",
    "    {'name': 'BLSTM (Termination Epoch: 300)' ,'list': auc_pr_blstm_arr_mean},\n",
    "    {'name': 'BLSTM (Termination Epoch: 1000)' ,'list': auc_pr_blstm_ext_arr_mean}\n",
    "]\n",
    "plot_auc_pr_per_layer(data, save=f'avg_auc_pr.{image_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'name': 'CNN (Kernel size: 3, Termination Epoch: 300)' ,'list': acc_cnn_arr_max},\n",
    "    {'name': 'BLSTM (Termination Epoch: 300)' ,'list': acc_blstm_arr_max},\n",
    "    {'name': 'BLSTM (Termination Epoch: 1000)' ,'list': acc_blstm_ext_arr_max}\n",
    "]\n",
    "plot_accuracy_per_layer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'name': 'CNN (Kernel size: 3, Termination Epoch: 300)' ,'list': f1_cnn_arr_max},\n",
    "    {'name': 'BLSTM (Termination Epoch: 300)' ,'list': f1_blstm_arr_max},\n",
    "    {'name': 'BLSTM (Termination Epoch: 1000)' ,'list': f1_blstm_ext_arr_max}\n",
    "]\n",
    "plot_f1_per_layer(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'name': 'CNN (Kernel size: 3, Termination Epoch: 300)' ,'list': auc_pr_cnn_arr_max},\n",
    "    {'name': 'BLSTM (Termination Epoch: 300)' ,'list': auc_pr_blstm_arr_max},\n",
    "    {'name': 'BLSTM (Termination Epoch: 1000)' ,'list': auc_pr_blstm_ext_arr_max}\n",
    "]\n",
    "plot_auc_pr_per_layer(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('dsd': venv)",
   "language": "python",
   "name": "python38664bitdsdvenv8f6b92adf1c54a1f8b3b505761591a36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}